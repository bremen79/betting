\section{The Betting Algorithm for Continous Coins}

\begin{algorithm}[t]
  \begin{algorithmic}
  {
    \STATE{\bfseries Parameters:} $a>2,\delta>1$
    \STATE{\bfseries Initialize:} $\gain_0=\epsilon,\sg_0=\delta$
    \FOR{$t=1,2,\dots$}
    \STATE{Calculate fraction and direction to bet: $\beta_t=2 \, S\left(\frac{4 \theta_{t-1}}{a (\sg_{t-1} + 1)}\right)-1$}
    \STATE{Bet $w_t=\beta_t \gain_{t-1}$}
    \STATE{Win (or lose) $w_t g_t$}
    \STATE{Update your money: $\gain_t=\gain_{t-1}+w_t g_t$}
    \STATE{Update $\theta_t$: $\theta_t=\theta_{t-1}+g_t$}
    \STATE{Update $\sg_t$: $\sg_t=\sg_{t-1}+|g_t|$}
    \ENDFOR
  }
  \end{algorithmic}
  \caption{Continous Coin Betting (COCOB)}
  \label{alg:cocob}
\end{algorithm}

As said in Section~\ref{}, portfolio selection can be used to optimally solve, betting, stochastic optimization, and self-tuning of regularized \ac{ERM}. However, no simple algorithms are known.
Hence, in this section we show how a very simple algorithm can achieve a bound close to the optimal one.

The \ac{COCOB} algorithm is shown in Algorithm~\ref{alg:cocob} and we can prove the following guarantee for its performance.
Without loss of generality, we will assume that $g_t$ are between $[-1,1]$.
%
\begin{theorem}
Let $a\geq2$ and start with an amount of money equal to $\epsilon$. Bet at each round a quantity equal to
$\gain_{t-1} \left(2 \, S\left(\frac{4 \theta_{t-1}}{a (\sg_{t-1} + 1)}\right)-1\right)$, where $S:\R\rightarrow(0,1)$ and $S(x)=\frac{1}{1+exp(-x)}$ and receive the outcome $g_t \in [-1,1]$. Then
\begin{align*}
\gain_{T} 
\geq \epsilon \exp\left(\frac{\theta_{T}^2}{a \sg_{T}} - \sum_{i=1}^{T} \frac{|g_i|}{a( \sg_{i-1} + 1) } \right)
\geq \epsilon \exp\left(\frac{\theta_{T}^2}{a \sg_{T}} - \sum_{i=1}^{T} \frac{|g_i|}{a( \sg_{i-1} + 1) } \right)~.
\end{align*}
\end{theorem}
%
\begin{proof}
%Define $\theta_{t-1}=\sum_{i=1}^t g_i$.

% \subsection{Data independent bound}
% 
% We will make use of the following lower bound
% \begin{lemma}
% There exist $a,b$ such that 
% $\log(1+x) \geq x - b x^2$, for $-2/a \leq x \leq 2/a$, $a>2$, $a\geq4 b$.
% \end{lemma}
% \begin{proof}
% For example, we can set $a>3.44$ and $b=a/4$ and verify the inequality numerically.
% \end{proof}
% Note that the inequality above is equivalent to $1+x \geq \exp(x - b x^2)$.
% 
% Assume that $L_{t-1}\geq \epsilon \exp(\frac{\theta_{t-1}^2}{a(t-1)}-\sum_{i=1}^{t-1} \frac{1}{a i})$.
% We have to prove that $L_{t}\geq \epsilon \exp(\frac{\theta_{t}^2}{a t}-\sum_{i=1}^{t} \frac{1}{a i})$.
% \begin{align}
% L_{t} &= L_{t-1} (1+\beta_t g_t) \\
% &\geq (1+\beta_t g_t) \epsilon \exp(\frac{\theta_{t-1}^2}{a(t-1)}-\sum_{i=1}^{t-1} \frac{1}{a i}) \\
% &=  \epsilon \exp(\frac{\theta_{t-1}^2}{a(t-1)}+\log(1+\beta_t g_t)-\sum_{i=1}^{t-1} \frac{1}{a i}).
% \end{align}
% Hence, we lower bound the quantity $\frac{\theta_{t-1}^2}{a(t-1)}+\log(1+\beta_t g_t) - \frac{\theta_{t}^2}{a t}-\sum_{i=1}^{t-1} \frac{1}{a i}$.
% \begin{align}
% &\frac{\theta_{t-1}^2}{a(t-1)}+\log(1+\beta_t g_t) -\frac{\theta_{t-1}^2 + 2 \theta_{t-1} g_t +g_t^2}{a t} -\sum_{i=1}^{t-1} \frac{1}{a i}\\ 
% & \quad = \frac{\theta_{t-1}^2}{a} (\frac{1}{t-1}-\frac{1}{t}) + \log(1+\beta_t g_t) -\frac{2 \theta_{t-1} g_t +g_t^2}{a t} -\sum_{i=1}^{t-1} \frac{1}{a i}\\
% & \quad \geq \frac{\theta_{t-1}^2}{a\,t\,(t-1)} + \log(1+\beta_t g_t) -\frac{2 \theta_{t-1} g_t}{a t} -\sum_{i=1}^{t} \frac{1}{a i}\\
% & \quad \geq \frac{\theta_{t-1}^2}{a\,t\,(t-1)} + \beta_t g_t - b \, (\beta_t g_t)^2 -\frac{\theta_{t-1} g_t}{a t} -\sum_{i=1}^{t} \frac{1}{a i}.
% \end{align}
% If we set $\beta_t=\frac{2 \theta_{t-1}}{a t}$ we have
% \begin{align}
% &\frac{\theta_{t-1}^2}{a(t-1)}+\log(1+\beta_t g_t) -\frac{\theta_{t-1}^2 + 2 \theta_{t-1} g_t +g_t^2}{a t} -\sum_{i=1}^{t-1} \frac{1}{a i}\\ 
% & \quad \geq \frac{\theta_{t-1}^2}{a\,t\,(t-1)}  - b\frac{4\theta_{t-1}^2}{a^2 t^2} -\sum_{i=1}^{t} \frac{1}{a i} \\
% & \quad \geq -\sum_{i=1}^{t} \frac{1}{a i}
% \end{align}
% where we used the fact that $a>4 b$ that is $1/a> 4 b/ a^2$.
% Hence, by induction, we have
% \begin{align}
% L_{T} &\geq \epsilon \exp(\frac{\theta_{T}^2}{a T}-\sum_{i=1}^{T} \frac{1}{a i}) \\
% &\geq \epsilon \exp(\frac{\theta_{T}^2}{a T} - \frac{1}{a}\log(T) -\frac{1}{a}) \\
% &=\frac{\epsilon \exp(-\frac{1}{a})}{T^\frac{1}{a}}\exp(\frac{\theta_{T}^2}{a T})
% \end{align}
% 
% \subsection{Data dependent bound}
% 
% Assume that $|g_t|\leq G_t$ and that you receive at each round $t$ the quantity $G_t$ before making the bet.
% 
% Define $a_t=2 \max_{i\leq t} G_i$, $\sg_t = \sum_{i=1}^t |g_i| + \delta$ and $\theta_t=\sum_{i=1}^t g_i$.
% 
% Assume that $L_{t-1}\geq \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t-1} \sg_{t-1}}- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i})$.
% We have to prove that $L_{t}\geq \epsilon \exp(\frac{\theta_{t}^2}{a_{t} \sg_t}- \sum_{i=1}^{t} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i}))$.
% \begin{align}
% L_{t} &= L_{t-1} (1+\beta_t g_t) \\
% &\geq (1+\beta_t g_t) \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t-1} \sg_{t-1}}- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i}) \\
% &=  \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t-1} \sg_{t-1}}+\log(1+\beta_t g_t)- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i}) \\
% &\geq  \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t} \sg_{t-1}}+\log(1+\beta_t g_t)- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i}).
% \end{align}
% Consider the function 
% \[
% \phi(x)=-\log(1+\beta_t x) + \frac{(\theta_{t-1}+x)^2}{a_t \sg_{t-1} + a_t |x|}.
% \]
% We have that $\phi(x)$ is piece-wise convex on $[-\infty,0]$ and $[0,\infty]$. Hence, we have that
% \begin{align}
% &\phi(x) \leq \phi(0)+\frac{x}{G_t} (\phi(G_t)-\phi(0)), \forall 0 \leq x\leq G_t\\
% &\phi(x) \leq \phi(0)+\frac{x}{G_t} (\phi(0)-\phi(-G_t)), \forall -G_t \leq x\leq 0.
% \end{align}
% 
% We now use set $\beta_t$ such that $\phi(G_t)=\phi(-G_t)$, that is
% \[
% \beta_t = \frac{1}{G_t} \frac{A_{t-1}-1}{A_{t-1}+1} 
% = \frac{1}{G_t} \left(2 \, sigmoid\left(\frac{4 \theta_{t-1} G_t}{a_{t} \sg_{t-1} + a_t G_t}\right)-1\right)
% \]
% where $A_{t-1}=\exp\left(\frac{4 \theta_{t-1} G_t}{a_{t} \sg_{t-1} + a_t G_t}\right)$ and
% $sigmoid (x) =\frac{1}{1+\exp(-x)}$.
% Hence we have
% \[
% \phi(x) \leq \phi(0)+\frac{|x|}{G_t} (\phi(G_t)-\phi(0)), \forall -G_t \leq x\leq G_t
% \]
% that is
% \begin{align}
% &\frac{\theta_{t-1}^2}{a_t \sg_{t-1}}-\frac{(\theta_{t-1}+x)^2}{a_t \sg_{t-1}+a_t |x|} + \log(1+\beta_t g_t) = \phi(0) - \phi(x) 
% \geq \frac{|x|}{G_t} (\phi(0) - \phi(G_t)) \\
% &\quad = \frac{|x|}{G_t} (\frac{\theta_{t-1}^2}{a_t \sg_{t-1}} - \frac{(\theta_{t-1}+G_t)^2}{a_t \sg_{t-1} + a_t G_t} + \log(1+\beta_t G_t)), \forall -G_t \leq x\leq G_t.
% \end{align}
% 
% 
% Using this relation we have that
% \begin{align}
% &-\frac{(\theta_{t-1}+g_t)^2}{a_t \sg_{t-1}+a_t |g_t|} + \frac{\theta_{t-1}^2}{a_t \sg_{t-1}}+\log(1+\beta_t g_t)-\sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i} \\
% &\quad \geq \frac{|g_t|}{G_t} (\frac{\theta_{t-1}^2}{a_t \sg_{t-1}} - \frac{(\theta_{t-1}+G_t)^2}{a_t \sg_{t-1} + a_t G_t} + \log(1+\beta_t G_t)) - \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i}\\
% &\quad = \frac{|g_t|}{G_t} (\frac{a_t G_t \theta_{t-1}^2 -2 G_t \theta_{t-1} \sg_{t-1} }{a_t \sg_{t-1}(a_t \sg_{t-1} + a_t G_t)} + \log(1+\beta_t G_t)) - \sum_{i=1}^{t} \frac{|g_i|G_i}{a_i \sg_{i-1} + a_i G_i}\\
% &\quad \geq \frac{|g_t|}{G_t} (\frac{a_t G_t \theta_{t-1}^2}{a_t \sg_{t-1}(a_t \sg_{t-1} + a_t G_t)}-\frac{2 G_t \theta_{t-1}}{a_t \sg_{t-1} + a_t G_t} + \log(1+\beta_t G_t)) - \sum_{i=1}^{t} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i}.
% \end{align}
% We now use the Taylor expansion, to obtain
% \[
% \log\left(1+\frac{\exp(x)-1}{\exp(x)+1}\right) \geq \frac{x}{2} -\frac{x^2}{8} \qquad \forall x \in \R
% \]
% and, using the expression of $\beta_t$, have
% \[
% \log\left(1+\beta_t G_t\right) = \log\left(1+\frac{\exp\left(\frac{4 \theta_{t-1} G_t}{a_t \sg_{t-1} + a_t G_t}\right)-1}{\exp\left(\frac{4 \theta_{t-1} G_t}{a_t \sg_{t-1} + a_t G_t}\right)+1}\right) \geq \frac{2 \theta_{t-1} G_t}{a_t \sg_{t-1} + a_t G_t} -\frac{2 \theta_{t-1}^2 G_t^2}{(a_t \sg_{t-1} + a_t G_t)^2}.
% \]
% Hence the expression 
% \[
% \frac{a_t G_t \theta_{t-1}^2}{a_t \sg_{t-1}(a_t \sg_{t-1} + a_t G_t)}-\frac{2 G_t \theta_{t-1}}{a_t \sg_{t-1} + a_t G_t} + \log(1+\beta_t G_t))
% \]
% is greater than zero if $a_t \geq 2 G_t$, that is true by definition of $a_t$.
% 
% By induction, the final lower bound is 
% \[
% L_{T} \geq \epsilon \exp\left(\frac{\theta_{T}^2}{a_T \sg_{T-1} + a_T G_T}-\sum_{i=1}^{T} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i} \right) \\
% \]

%\subsection{Data dependent bound, version 2}

From the assumptions we have that $|g_t|\leq 1$.

%Define $\sg_t = \sum_{i=1}^t |g_i| G_i + \delta$ and $\theta_t=\sum_{i=1}^t g_i$.
We will prove by induction a lower bound on $\gain_T$, hence our induction hypothesis is
\[
\gain_{t-1}\geq \epsilon \exp\left(\frac{\theta_{t-1}^2}{a \sg_{t-1}}- \sum_{i=1}^{t-1} \frac{|g_i|}{a (\sg_{i-1} + 1)}\right)~.
\]
We have to prove that 
\[
\gain_{t}\geq \epsilon \exp\left(\frac{\theta_{t}^2}{a \sg_t}- \sum_{i=1}^{t} \frac{|g_i| G_i}{a (\sg_{i-1} + 1)}\right)~.
\]

Let's start calculating the value of the reward at time $t$.
\begin{align*}
\gain_{t} &= \gain_{t-1} (1+\beta_t g_t) \\
&\geq \epsilon (1+\beta_t g_t)\exp\left(\frac{\theta_{t-1}^2}{a \sg_{t-1}}- \sum_{i=1}^{t-1} \frac{|g_i|}{a (\sg_{i-1} + 1) }\right) \\
&=  \epsilon \exp\left(\frac{\theta_{t-1}^2}{a \sg_{t-1}}+\log(1+\beta_t g_t)- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a (\sg_{i-1} + 1)}\right)~.
\end{align*}

Now, consider the function 
\[
\phi(x)=-\log(1+\beta_t x) + \frac{(\theta_{t-1}+x)^2}{a (\sg_{t-1} + |x|)}~.
\]
We have that $\phi(x)$ is piece-wise convex on $[-\infty,0]$ and $[0,\infty]$. Hence, we have that
\begin{align*}
&\phi(x) \leq \phi(0)+x (\phi(1)-\phi(0)), \forall 0 \leq x\leq 1\\
&\phi(x) \leq \phi(0)+x (\phi(0)-\phi(-1)), \forall -1 \leq x\leq 0~.
\end{align*}
Also, we set $\beta_t$ such that $\phi(1)=\phi(-1)$, that is
\[
\beta_t = \frac{A_{t-1}-1}{A_{t-1}+1} 
= 2 \, S\left(\frac{4 \theta_{t-1}}{a (\sg_{t-1} + 1)}\right)-1
\]
where $A_{t-1}=\exp\left(\frac{4 \theta_{t-1}}{a (\sg_{t-1} + 1)}\right)$ and
$S(x) =\frac{1}{1+\exp(-x)}$.
Hence we have
\[
\phi(x) \leq \phi(0)+ |x| (\phi(1)-\phi(0)), \forall -1 \leq x\leq 1,
\]
that is
\begin{align*}
\frac{\theta_{t-1}^2}{a \sg_{t-1}}-\frac{(\theta_{t-1}+x)^2}{a (\sg_{t-1} +  |x|)} + \log(1+\beta_t g_t) 
& = \phi(0) - \phi(x) \\
& \geq |x| \left(\phi(0) - \phi(1)\right) \\
&= |x| \left(\frac{\theta_{t-1}^2}{a \sg_{t-1}} - \frac{(\theta_{t-1}+G_t)^2}{a (\sg_{t-1} + 1)} + \log(1+\beta_t)\right), \forall -1 \leq x\leq 1~.
\end{align*}

Using this relation we have that
\begin{align*}
&-\frac{(\theta_{t-1}+g_t)^2}{a (\sg_{t-1}+|g_t|)} + \frac{\theta_{t-1}^2}{a \sg_{t-1}}+\log(1+\beta_t g_t)-\sum_{i=1}^{t-1} \frac{|g_i|}{a (\sg_{i-1} + 1)} \\
&\qquad \geq |g_t| \left(\frac{\theta_{t-1}^2}{a \sg_{t-1}} - \frac{(\theta_{t-1}+1)^2}{a (\sg_{t-1} + 1)} + \log(1+\beta_t G_t)\right) - \sum_{i=1}^{t-1} \frac{|g_i|}{a (\sg_{i-1} + 1)}\\
&\qquad = |g_t| \left(\frac{a \theta_{t-1}^2 -2 \theta_{t-1} \sg_{t-1} }{a^2 \sg_{t-1}(\sg_{t-1} + 1)} + \log(1+\beta_t G_t)\right) - \sum_{i=1}^{t} \frac{|g_i|}{a (\sg_{i-1} + 1)}\\
&\qquad \geq |g_t| \left(\frac{a \theta_{t-1}^2}{a^2 \sg_{t-1}(\sg_{t-1} + 1)}-\frac{2 \theta_{t-1}}{a (\sg_{t-1} + 1)} + \log(1+\beta_t)\right) - \sum_{i=1}^{t} \frac{|g_i|}{a (\sg_{i-1} + 1)}~.
\end{align*}
We now use the Taylor expansion, to obtain
\[
\log\left(1+\frac{\exp(x)-1}{\exp(x)+1}\right) \geq \frac{x}{2} -\frac{x^2}{8} \qquad \forall x \in \R
\]
and, using the expression of $\beta_t$, have
\[
\log\left(1+\beta_t\right) 
= \log\left(1+\frac{\exp\left(\frac{4 \theta_{t-1}}{a (\sg_{t-1} + 1)}\right)-1}{\exp\left(\frac{4 \theta_{t-1}}{a (\sg_{t-1} + 1)}\right)+1}\right) 
\geq \frac{2 \theta_{t-1}}{a (\sg_{t-1} + 1)} -\frac{2 \theta_{t-1}^2}{a^2 (\sg_{t-1} + 1)^2}.
\]
Hence the expression 
\[
\frac{a \theta_{t-1}^2}{a^2 \sg_{t-1}(\sg_{t-1} + 1)}-\frac{2 \theta_{t-1}}{a (\sg_{t-1} + 1)} + \log(1+\beta_t))
\]
is greater than zero if $a \geq 2$, that is true by definition of $a$.

Hence, by induction, the stated bound on $\gain_T$ is proved.
\end{proof}

% \subsection{Data dependent bound, version 3}
% 
% Assume that $|g_t|\leq G_t$ and that you receive at each round $t$ the quantity $G_t$ before making the bet.
% 
% Define $\sg_t = \sum_{i=1}^t |g_i| G_i$ and $\theta_t=\sum_{i=1}^t g_i$.
% 
% Assume that $L_{t-1}\geq \epsilon \exp(\frac{\theta_{t-1}^2}{a (\sg_{t-1} + \delta_{t-1})}- \frac{1}{a}\sum_{i=1}^{t-1} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_{i}})$.
% We have to prove that $L_{t}\geq \epsilon \exp(\frac{\theta_{t}^2}{a (\sg_t + \delta_t)}- \frac{1}{a}\sum_{i=1}^{t} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_i}))$.
% \begin{align}
% L_{t} &= L_{t-1} (1+\beta_t g_t) \\
% &\geq (1+\beta_t g_t) \epsilon \exp(\frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_{t-1})}- \frac{1}{a}\sum_{i=1}^{t-1} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_i}) \\
% &=  \epsilon \exp(\frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_{t-1})}+\log(1+\beta_t g_t)- \frac{1}{a}\sum_{i=1}^{t-1} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i +\delta_i}) .
% \end{align}
% Consider the function 
% \[
% \phi(x)=-\log(1+\beta_t x) + \frac{(\theta_{t-1}+x)^2}{a \sg_{t-1} + a |x| G_t +a \delta_t}.
% \]
% We have that $\phi(x)$ is piece-wise convex on $[-\infty,0]$ and $[0,\infty]$. Hence, we have that
% \begin{align}
% &\phi(x) \leq \phi(0)+\frac{x}{G_t} (\phi(G_t)-\phi(0)), \forall 0 \leq x\leq G_t\\
% &\phi(x) \leq \phi(0)+\frac{x}{G_t} (\phi(0)-\phi(-G_t)), \forall -G_t \leq x\leq 0.
% \end{align}
% 
% We now use set $\beta_t$ such that $\phi(G_t)=\phi(-G_t)$, that is
% \[
% \beta_t = \frac{1}{G_t} \frac{A_{t-1}-1}{A_{t-1}+1} 
% = \frac{1}{G_t} \left(2 \, sigmoid\left(\frac{4 \theta_{t-1} G_t}{a \sg_{t-1} + a G_t^2 + a \delta_t}\right)-1\right)
% \]
% where $A_{t-1}=\exp\left(\frac{4 \theta_{t-1} G_t}{a_{t} \sg_{t-1} + a G_t + a \delta_t }\right)$ and
% $sigmoid (x) =\frac{1}{1+\exp(-x)}$.
% Hence we have
% \[
% \phi(x) \leq \phi(0)+\frac{|x|}{G_t} (\phi(G_t)-\phi(0)), \forall -G_t \leq x\leq G_t
% \]
% that is
% \begin{align}
% &\frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_{t-1})}-\frac{(\theta_{t-1}+x)^2}{a \sg_{t-1} + a |x| G_t + a\delta_t} + \log(1+\beta_t g_t) \\
% &\quad \geq \frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_{t})}-\frac{(\theta_{t-1}+x)^2}{a \sg_{t-1} + a |x| G_t + a\delta_t} + \log(1+\beta_t g_t) \\
% &\quad = \phi(0) - \phi(x) 
% \geq \frac{|x|}{G_t} (\phi(0) - \phi(G_t)) \\
% &\quad = \frac{|x|}{G_t} (\frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_t)} - \frac{(\theta_{t-1}+G_t)^2}{a \sg_{t-1} + a G_t^2 + a \delta_t} + \log(1+\beta_t G_t)), \forall -G_t \leq x\leq G_t.
% \end{align}
% 
% Using this relation we have that
% \begin{align}
% &-\frac{(\theta_{t-1}+g_t)^2}{a (\sg_{t-1}+|g_t| G_t+\delta_t)} + \frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_{t-1})}+\log(1+\beta_t g_t)-\frac{1}{a}\sum_{i=1}^{t-1} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_i} \\
% &\quad \geq \frac{|g_t|}{G_t} (\frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_t)} - \frac{(\theta_{t-1}+G_t)^2}{a (\sg_{t-1} + G^2_t+ \delta_t)} + \log(1+\beta_t G_t)) - \frac{1}{a}\sum_{i=1}^{t-1} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_i}\\
% &\quad = \frac{|g_t|}{G_t} (\frac{G^2_t \theta_{t-1}^2 -2 G_t \theta_{t-1} (\sg_{t-1} +\delta_t)}{a (\sg_{t-1}+\delta_t)(\sg_{t-1} + G^2_t +\delta_t)} + \log(1+\beta_t G_t)) - \frac{1}{a}\sum_{i=1}^{t} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_i}\\
% &\quad = \frac{|g_t|}{G_t} (\frac{G^2_t \theta_{t-1}^2}{a (\sg_{t-1}+\delta_t)(\sg_{t-1} + G^2_t +\delta_t)}-\frac{2 G_t \theta_{t-1}}{a (\sg_{t-1} + G^2_t+\delta_t)} + \log(1+\beta_t G_t)) - \frac{1}{a}\sum_{i=1}^{t} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_i}.
% \end{align}
% We now use the Taylor expansion, to obtain
% \[
% \log\left(1+\frac{\exp(x)-1}{\exp(x)+1}\right) \geq \frac{x}{2} -\frac{x^2}{8} \qquad \forall x \in \R
% \]
% and, using the expression of $\beta_t$, have
% \[
% \log\left(1+\beta_t G_t\right) 
% = \log\left(1+\frac{\exp\left(\frac{4 \theta_{t-1} G_t}{a (\sg_{t-1} + G^2_t+\delta_t)}\right)-1}{\exp\left(\frac{4 \theta_{t-1} G_t}{a (\sg_{t-1} + G^2_t+\delta_t)}\right)+1}\right) 
% \geq \frac{2 \theta_{t-1} G_t}{a (\sg_{t-1} + G^2_t+\delta_t)} -\frac{2 \theta_{t-1}^2 G_t^2}{a^2 (\sg_{t-1} + G^2_t+\delta_t)^2}.
% \]
% Hence the expression 
% \[
% \frac{G^2_t \theta_{t-1}^2}{a (\sg_{t-1}+\delta_t)(\sg_{t-1} + G^2_t +\delta_t)}-\frac{2 G_t \theta_{t-1}}{a (\sg_{t-1} + G^2_t+\delta_t)} + \log(1+\beta_t G_t))
% \]
% is greater than zero if $a \geq 2$, that is true by definition of $a$.
% 
% By induction, the final lower bound is 
% \[
% L_{T} \geq \epsilon \exp\left(\frac{\theta_{T}^2}{a (\sg_{T}+\delta_T)} - \frac{1}{a}\sum_{i=1}^{T} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i +\delta_i} \right) \\
% \]
% 
% %We now upper bound the last term in the last inequality:
% %\begin{align}
% %\sum_{i=1}^{T} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i +\delta_i}
% %&\leq \sum_{i=1}^{T} \frac{|g_i| G_i}{\sg_{i} + \delta_i} \\
% %&\leq \log\left(1+ \frac{\sum_{i=1}^t |g_i| G_i}{\delta_T}\right) + \log\frac{\delta_T}{\delta_1}.
% %\end{align}
% 
% We can set $\delta_t$ such that $\frac{|g_i| G_i}{\sg_{i-1} + G^2_i +\delta_i}\leq \frac{1}{i}$ and $\delta_i$ are increasing.
% 
% 
% \subsection{Data dependent bound, no $\delta$}
% 
% Assume that $|g_t|\leq G_t$ and that you receive at each round $t$ the quantity $G_t$ before making the bet.
% 
% Define $a_t=2 \max_{i\leq t} G_i^2$, $\sg_t = \sum_{i=1}^t \frac{|g_i|}{G_t} + 1$ and $\theta_t=\sum_{i=1}^t g_i$.
% 
% Assume that $L_{t-1}\geq \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t-1} \sg_{t-1}}- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i})$.
% We have to prove that $L_{t}\geq \epsilon \exp(\frac{\theta_{t}^2}{a_{t} \sg_t}- \sum_{i=1}^{t} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i}))$.
% \begin{align}
% L_{t} &= L_{t-1} (1+\beta_t g_t) \\
% &\geq (1+\beta_t g_t) \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t-1} \sg_{t-1}}- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i }) \\
% &=  \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t-1} \sg_{t-1}}+\log(1+\beta_t g_t)- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i}) \\
% &\geq  \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t} \sg_{t-1}}+\log(1+\beta_t g_t)- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i}).
% \end{align}
% Consider the function 
% \[
% \phi(x)=-\log(1+\beta_t x) + \frac{(\theta_{t-1}+x)^2}{a_t \sg_{t-1} + a_t \frac{|x|}{G_t}}.
% \]
% We have that $\phi(x)$ is piece-wise convex on $[-\infty,0]$ and $[0,\infty]$. Hence, we have that
% \begin{align}
% &\phi(x) \leq \phi(0)+\frac{x}{G_t} (\phi(G_t)-\phi(0)), \forall 0 \leq x\leq G_t\\
% &\phi(x) \leq \phi(0)+\frac{x}{G_t} (\phi(0)-\phi(-G_t)), \forall -G_t \leq x\leq 0.
% \end{align}
% 
% We now use set $\beta_t$ such that $\phi(G_t)=\phi(-G_t)$, that is
% \[
% \beta_t = \frac{1}{G_t} \frac{A_{t-1}-1}{A_{t-1}+1} 
% = \frac{1}{G_t} \left(2 \, sigmoid\left(\frac{4 \theta_{t-1} G_t}{a_{t} \sg_{t-1} + a_t}\right)-1\right)
% \]
% where $A_{t-1}=\exp\left(\frac{4 \theta_{t-1} G_t}{a_{t} \sg_{t-1} + a_t}\right)$ and
% $sigmoid (x) =\frac{1}{1+\exp(-x)}$.
% Hence we have
% \[
% \phi(x) \leq \phi(0)+\frac{|x|}{G_t} (\phi(G_t)-\phi(0)), \forall -G_t \leq x\leq G_t
% \]
% that is
% \begin{align}
% &\frac{\theta_{t-1}^2}{a_t \sg_{t-1}}-\frac{(\theta_{t-1}+x)^2}{a_t \sg_{t-1}+a_t \frac{|x|}{G_t}} + \log(1+\beta_t g_t) = \phi(0) - \phi(x) 
% \geq \frac{|x|}{G_t} (\phi(0) - \phi(G_t)) \\
% &\quad = \frac{|x|}{G_t} (\frac{\theta_{t-1}^2}{a_t \sg_{t-1}} - \frac{(\theta_{t-1}+G_t)^2}{a_t \sg_{t-1} + a_t} + \log(1+\beta_t G_t)), \forall -G_t \leq x\leq G_t.
% \end{align}
% 
% 
% Using this relation we have that
% \begin{align}
% &-\frac{(\theta_{t-1}+g_t)^2}{a_t \sg_{t-1}+a_t \frac{|g_t|}{G_t}} + \frac{\theta_{t-1}^2}{a_t \sg_{t-1}}+\log(1+\beta_t g_t)-\sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i} \\
% &\quad \geq \frac{|g_t|}{G_t} (\frac{\theta_{t-1}^2}{a_t \sg_{t-1}} - \frac{(\theta_{t-1}+G_t)^2}{a_t \sg_{t-1} + a_t} + \log(1+\beta_t G_t)) - \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i}\\
% &\quad = \frac{|g_t|}{G_t} (\frac{\theta_{t-1}^2 -2 G_t \theta_{t-1} \sg_{t-1} }{a_t \sg_{t-1}(\sg_{t-1} + 1)} + \log(1+\beta_t G_t)) - \sum_{i=1}^{t} \frac{|g_i|G_i}{a_i \sg_{i-1} + a_i}\\
% &\quad = \frac{|g_t|}{G_t} (\frac{\theta_{t-1}^2}{a_t \sg_{t-1}(\sg_{t-1} + 1)}-\frac{2 G_t \theta_{t-1}}{a_t \sg_{t-1} + a_t} + \log(1+\beta_t G_t)) - \sum_{i=1}^{t} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i}.
% \end{align}
% We now use the Taylor expansion, to obtain
% \[
% \log\left(1+\frac{\exp(x)-1}{\exp(x)+1}\right) \geq \frac{x}{2} -\frac{x^2}{8} \qquad \forall x \in \R
% \]
% and, using the expression of $\beta_t$, have
% \[
% \log\left(1+\beta_t G_t\right) = \log\left(1+\frac{\exp\left(\frac{4 \theta_{t-1} G_t}{a_t \sg_{t-1} + a_t}\right)-1}{\exp\left(\frac{4 \theta_{t-1} G_t}{a_t \sg_{t-1} + a_t}\right)+1}\right) \geq \frac{2 \theta_{t-1} G_t}{a_t \sg_{t-1} + a_t} -\frac{2 \theta_{t-1}^2 G_t^2}{(a_t \sg_{t-1} + a_t)^2}.
% \]
% Hence the expression 
% \[
% \frac{\theta_{t-1}^2}{a_t \sg_{t-1}(\sg_{t-1} + 1)}-\frac{2 G_t \theta_{t-1}}{a_t \sg_{t-1} + a_t} + \log(1+\beta_t G_t))
% \]
% is greater than zero if $a_t \geq 2 G_t^2$, that is true by definition of $a_t$.
% 
% By induction, the final lower bound is 
% \begin{align}
% L_{T} 
% &\geq \epsilon \exp\left(\frac{\theta_{T}^2}{a_T \sg_{T}}-\sum_{i=1}^{T} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i} \right) \\
% &\geq \epsilon \exp\left(\frac{\theta_{T}^2}{2 G_T^2 \sg_{T}}-\frac{1}{2}\sum_{i=1}^{T} \frac{\frac{|g_i|}{G_i}}{\sg_{i}} \right) \\
% &\geq \epsilon \exp\left(\frac{\theta_{T}^2}{2 G_T^2 \sg_{T}} - \frac{1}{2} \log\left(1+ \sum_{i=1}^{T} \frac{|g_i|}{G_i} \right) \right)
% \end{align}
% 
% It is interesting to note that $\beta_t$ is \emph{not} between $-1$ and $1$, but between $-1/G_t$ and $1/G_t$.
% 
% The ideal case of $\exp\left(\frac{\theta_T^2}{\sum_{t=1}^T g_t^2}\right)$ is equal to
% \[
% \exp\left(\frac{\theta_T^2}{T\frac{\sum_{t=1}^T g_t^2}{T}}\right) 
% \approx \exp\left(\frac{\theta_T^2}{\left(\sum_{t=1}^T \frac{|g_t|}{G_t}\right) \frac{\sum_{t=1}^T g_t^2}{T}}\right)
% \approx \exp\left(\frac{\theta_T^2}{\left(\sum_{t=1}^T \frac{|g_t|}{G_t}\right) \max_{t \leq T} g_t^2 }\right)
% \]
