\appendix
\section{Proofs}
\label{sec:proofs}

First we state some technical lemmas that will be used in the following proofs.

\begin{lemma}
The Lambert function satisfies $W(x) \geq 0.6321 \log(x+1), \forall x\geq0$.
\end{lemma}
\begin{proof}
The Lambert function satisfies
\[
x=W(x) \exp \left(W(x)\right).
\]
Hence,
\begin{align}
W(x) &= \log\left(\frac{x}{W(x)}\right) \label{eq:lm_lambert_1} \\
&= \log\left(\frac{x}{\log(x/W(x))}\right). \label{eq:lm_lambert_1b}
\end{align}
From the first equality, for any $a>0$, we get
\[
W(x) \leq \frac{1}{a\, e}\left(\frac{x}{W(x)}\right)^a
\]
that is
\begin{equation}
\label{eq:lm_lambert_2}
W(x) \leq \left(\frac{1}{a\, e}\right)^\frac{1}{1+a} x^\frac{a}{1+a}.
\end{equation}
Using \eqref{eq:lm_lambert_2} in \eqref{eq:lm_lambert_1}, we have
\begin{align*}
W(x) 
\geq \log\left(\frac{x}{\left(\frac{1}{a\, e}\right)^\frac{1}{1+a} x^\frac{a}{1+a}}\right) 
= \frac{1}{1+a}\log\left(a \, e\, x\right)~.
\end{align*}
Consider now the function $g(x)=\frac{x}{x+1} - \frac{b}{\log(1+b) (b+1)} \log(x+1), x\geq b$. This function has a maximum in $x^*=(1+\frac{1}{b}) \log(1+b)-1$, the derivative is positive in $[0,x^*]$ and negative in $[x^*,b]$. Hence the minimum is in $x=0$ and in $x=b$, where it is equal to $0$.
Using the property just proved on $g$, we have that for $x\leq b$, setting $a=\frac{1}{x}$, we have
\begin{align*}
W(x) 
\geq \frac{x}{x+1} \geq \frac{b}{\log(1+b) (b+1)} \log(x+1)~.
\end{align*}
For $x>b$, setting $a=\frac{x+1}{e x}$, we have
\begin{align}
W(x) 
&\geq \frac{e\,x}{(e+1) x + 1} \log(x+1) \geq \frac{e\,b}{(e+1) b + 1} \log(x+1)
\end{align}
Hence, we set $b$ such that 
\[
\frac{e\, b}{(e+1)b + 1} = \frac{b}{\log(1+b) (b+1)}
\]
Numberically, $b=1.71825...$, so
\[
W(x) \geq 0.6321 \log(x+1)~. \qedhere
\]
\end{proof}

\begin{lemma}
Define $f(\theta)= \beta \exp\frac{x^2}{2 \alpha}$, for $\alpha,\beta>0$, $x\geq0$. Then
\[
f^*(y)=y \sqrt{\alpha W\left(\frac{\alpha y^2}{\beta^2}\right)} - \beta \exp\left(\frac{W\left(\frac{\alpha y^2}{\beta^2}\right)}{2}\right).
\]
Moreover
\[
f^*(y) \leq y \sqrt{\alpha \log \left(\frac{\alpha y^2}{\beta^2} +1 \right)} - \beta.
\]
\end{lemma}
\begin{proof}
From the definition of Fenchel dual, we have
\begin{align*}
f^*(y)= \max_{x} \  x\, y - f(x) = \max_{x} \  x\, y - \beta \exp\frac{x^2}{2 \alpha} \leq x^*\,y -\beta
\end{align*}
where $x^*= \argmax_{x} x\, y - f(x)$. We now use the fact that $x^*$ satisfies $y = f'(x^*)$, to have
\begin{align*}
x^*=\sqrt{\alpha W\left(\frac{\alpha y^2}{\beta^2}\right)},
\end{align*}
where the function $W:\R_+ \rightarrow \R$ is the Lambert function that satisfies
\[
x=W(x) \exp \left(W(x)\right).
\]
Hence, to obtain an upper bound we need an upper bound to the Lambert function.
We use Theorem~2.3 in \cite{hoorfar2008inequalities}, that says that
\[
W(x) \leq \log\frac{x+C}{1+\log(C)}, \quad \forall x> -\frac{1}{e}, \ C>\frac{1}{e}.
\]
Setting $C=1$, we obtain the stated bound.
\end{proof}

\begin{lemma}[ {\citep[Example 13.7]{BauschkeC2011}} ]
Let $\phi:\R \rightarrow (-\infty, +\infty]$ be even. Then $(\phi \ast \norm{\cdot})^*=\phi^* \ast \norm{\cdot}$.
\end{lemma}

\begin{cor}
Define $f(\btheta)= \beta \exp\frac{\norm{\btheta}^2}{2 \alpha}$, for $\alpha,\beta>0$. Then
\[
f^*(y) \leq  \norm{\btheta} \sqrt{\alpha \log \left(\frac{\alpha \norm{\btheta}^2}{\beta^2} +1 \right)} - \beta.
\]
\end{cor}

\subsection{Proofs of Theorem~\ref{thm:oracle_fraction} and Theorem~\ref{thm:oracle_fraction_changing}}

We first state a couple of useful indentities.
For any $ 0\leq p < 1$
\[
D\left(\frac{1}{2}+\frac{p}{2}\middle\|\frac{1}{2}\right) = D\left(\frac{1}{2}-\frac{p}{2}\middle\|\frac{1}{2}\right)= \frac{1+p}{2} \log(1+p) + \frac{1-p}{2} \log(1-p).
\]
The extension for continuity of $D(\frac{1}{2}+\frac{p}{2}||\frac{1}{2})$ in $p=1$ is $\log(2)$.
Also,
\[
\left(\frac{n}{n-q}\right)^{n-q} \left(\frac{n}{q}\right)^{q} = 2^n \exp\left(-n D\left(\frac{q}{n}\middle\|\frac{1}{2}\right)\right),
\]
and
\begin{equation}
\label{eq:div_2}
\left(1+x\right)^\frac{1+x}{2} \left(1-x\right)^\frac{1-x}{2}= \exp\left( D\left(\frac{1}{2}+\frac{x}{2}\middle\|\frac{1}{2}\right) \right)
\end{equation}

Also, for any $-\frac{1}{2} \leq x\leq \frac{1}{2}$ we have
\[
\frac{x^2}{2} +\frac{x^4}{12}\leq D\left(\frac{1}{2}+\frac{x}{2}\middle\|\frac{1}{2}\right) \leq \frac{x^2}{2} + \frac{x^4}{5}.
\]

\begin{proof}[Proof of Theorem~\ref{thm:oracle_fraction}]
From the betting strategy we have
\[
\gain_t=\gain_{t-1} + w_t \, g_t = \gain_{t-1} + \beta \, \gain_{t-1} \, g_t = \gain_{t-1} (1+\beta \, g_t)~.
\]
Hence
\[
\gain_n=\epsilon \prod_{t=1}^n (1+\beta g_t) = \epsilon (1+\beta)^\frac{n+Z}{2} (1-\beta)^\frac{n-Z}{2},
\]
where $G=\sum_{t=1}^n g_t$.
It is easy to show that the maximum value of $\gain_n$ w.r.t. $\beta$ is in $\beta=\frac{G}{n}$. 
Hence, we have
\[
\gain_n = \epsilon \left(1+\frac{G}{n}\right)^\frac{n+G}{2} \left(1-\frac{G}{n}\right)^\frac{n-G}{2} 
= \epsilon \left[\left(1+\frac{G}{n}\right)^\frac{1+\frac{G}{n}}{2} \left(1-\frac{G}{n}\right)^\frac{1-\frac{G}{n}}{2}\right]^n = \exp\left( D\left(\frac{1}{2}+\frac{G}{2n}\middle\|\frac{1}{2}\right) \right),
%\leq \epsilon \exp \left(\frac{Z^2}{2 n} + \frac{Z^4}{5 n^3}\right),
\]
where in the last equality we used \eqref{eq:div_2}.
%or 
%\[
%\frac{x^2}{2} +\frac{x^4}{12}\leq \frac{1+x}{2} \log(1+x) + \frac{1-x}{2}\log(1-x) \leq \frac{x^2}{2} + \log(2)-.5
%\]
%where the lhs is given by Taylor expansion.
\end{proof}

\begin{theorem}
\label{lemma:bin}
Let $n\geq2$ an even number of Bernoulli random variables $b_i$. Then for any $k \in \Nat_0$ such that $k\leq \frac{1}{2}n-1$, we have
\[
P\left( \sum_{i=1}^n b_i \geq \frac{1}{2} n + k\right) 
\geq  \frac{\exp\left(-n\, D\left(\frac{1}{2}+\frac{k}{n}\middle\|\frac{1}{2}\right)\right)}{2 \exp\left(\frac{1}{6}\right)} \frac{\sqrt{2 \pi}}{(\pi-1)y+\sqrt{y^2+2 \pi}},
\]
where $y=\frac{2 k}{\sqrt{n}}$.
\end{theorem}
\begin{proof}
We use Theorem~2 in \cite{McKay1989}, that specialized to our case says that
\begin{equation}
\label{eq:bin_1}
P\left( \sum_{i=1}^n b_i \geq  \frac{1}{2} n + k  \right) 
\geq \sqrt{n} \binom{n-1}{ \frac{1}{2} n + k -1} 2^{-n} \frac{Q(y)}{\phi(y)},
\end{equation}
where $\phi(x)$ is the unit variance, zero mean Gaussian, $\frac{1}{\sqrt{2 \pi}} \exp(-\frac{x^2}{2})$ and $Q(x)$ is its CDF, $\int_{x}^{+\infty} \phi(u) du$.

We start lower bounding the ratio $\frac{Q(y)}{\phi(y)}$. Using the inequality in \cite{Boyd59}, that says
\[
\frac{Q(y)}{\phi(y)} 
= \exp\left(\frac{x^2}{2}\right) \int_{x}^{+\infty} \exp\left(-\frac{t^2}{2}\right) dt
\geq \frac{\pi}{(\pi-1)x+\sqrt{x^2+2 \pi}}.
\]

To bound the binomial coefficient we make use of the following Stirling approximation, for any $n\geq 1$,
\[
\sqrt{2 \pi n} n^n \exp(-n) < n! < \exp\left(\frac{1}{12}\right)\sqrt{2 \pi n} n^n \exp(-n)~.
\]
Hence, for any $n \geq 2$ and $1\leq q \leq n-1$, after some algebra we obtain
\begin{align*}
{n \choose q} 
&\geq \frac{1}{\exp\left(\frac{1}{6}\right) \sqrt{2 \pi}} \left(\frac{n}{n-q}\right)^{n-q} \left(\frac{n}{q}\right)^{q} \sqrt{\frac{n}{q(n-q)}} \\
&\geq \frac{1}{\exp\left(\frac{1}{6}\right) \sqrt{2 \pi}} 2^n \exp\left(-n D\left(\frac{q}{n}\middle\|\frac{1}{2}\right)\right) \sqrt{\frac{n}{q(n-q)}}.
\end{align*}
where in the equality we used the definition of $D\left(\cdot\middle\|\cdot\right)$.
Also, we have
\begin{equation}
\label{eq:bin_3}
{n-1 \choose \frac{1}{2} n + k - 1} = {n \choose \frac{1}{2} n + k} \left(\frac{1}{2} + \frac{k}{n}\right) .
\end{equation}
Putting together \eqref{eq:bin_1}-\eqref{eq:bin_3}, and using the definition of $y$ we have
\begin{align*}
P\left( \sum_{i=1}^n b_i \geq \frac{1}{2} n + k \right) 
&\geq \frac{1}{\exp\left(\frac{1}{6}\right) \sqrt{2 \pi}} \exp\left(-n\, D\left(\frac{1}{2}+\frac{k}{n}\middle\|\frac{1}{2}\right)\right) \sqrt{\frac{\frac{1}{2} + \frac{k}{n}}{\frac{1}{2}-\frac{k}{n}}}  \frac{Q(y)}{\phi(y)} \\
&\geq \frac{1}{\exp\left(\frac{1}{6}\right) \sqrt{2 \pi}} \exp\left(-n\, D\left(\frac{1}{2}+\frac{k}{n}\middle\|\frac{1}{2}\right)\right) \frac{\pi}{(\pi-1)y+\sqrt{y^2+2 \pi}}. \qedhere
\end{align*}
\end{proof}

We can now prove Theorem~\ref{thm:oracle_fraction_changing}.

\begin{proof}[Proof of Theorem~\ref{thm:oracle_fraction_changing}]
First observe that, even knowing all the outcomes of $g_t$, we cannot gain more than $\epsilon 2^n$, simply betting on each round all the money on the correct outcome.

Then, for a specific function $g(\cdot,\cdot)$ that grows in the first argument, we will show that
\begin{align*}
\min_{|\sum_{t=1}^n g_t| \geq 2 k} \sum_{t=1}^n w_t g_t 
\leq \epsilon \, g(k,n), \ \forall 0 \leq k \leq n-1.
\end{align*}
When $k=n/2$, we cannot gain more than $\epsilon 2^n$, this is why we can safely consider only $0\leq k\leq n/2-1$.
From this we will infer that
\begin{align*}
\min_{g_t} \sum_{t=1}^n w_t g_t - \epsilon\min\left(g\left(\frac{|\sum_{t=1}^n g_t|}{2},n\right),2^n\right) \leq 0,
\end{align*}
that implies the stated inequality.

Set $r_t$ as independent random variable that assumes the value of 1 with probability 0.5 and -1 with probability 0.5.
Hence, we have that $\E[ \sum_{t=1}^n w_t r_t ]=0$, and also $\sum_{t=1}^n w_t r_t \geq -\epsilon$ because we never lose more than the initial amount of money.

Hence we have
\begin{align*}
\min_{|\sum_{t=1}^n g_t| \geq 2 k} \sum_{t=1}^n w_t g_t 
\leq \E\left[\sum_{t=1}^n w_t r_t \bigg| \left|\sum_{t=1}^n r_t\right| \geq 2 k\right], \ \forall 0 \leq k \leq n-1.
\end{align*}

For any $k\geq 0$, it follows that
\begin{align*}
0&=\E\left[\sum_{t=1}^n w_t r_t \right] 
= \E\left[\sum_{t=1}^n w_t r_t \bigg| \left|\sum_{t=1}^n r_t \right|< 2 k\right] P\left(\left|\sum_{t=1}^n r_t\right|< 2k \right)\\
&\qquad+\E\left[\sum_{t=1}^n w_t r_t \bigg| \left|\sum_{t=1}^n r_t \right| \geq 2 k\right] P\left(\left|\sum_{t=1}^n r_t\right|\geq 2 k\right) \\
&=\E\left[\sum_{t=1}^n w_t r_t \bigg| \left|\sum_{t=1}^n r_t \right|< 2 k\right] (1-P\left(\left|\sum_{t=1}^n r_t\right|\geq 2 k\right))\\
&\qquad+\E\left[\sum_{t=1}^n w_t r_t \bigg| \left|\sum_{t=1}^n r_t \right| \geq 2 k\right] P\left(\left|\sum_{t=1}^n r_t\right|\geq 2 k\right) \\
&\geq -\epsilon (1-P\left(\left|\sum_{t=1}^n r_t\right|\geq 2 k\right))+\E\left[\sum_{t=1}^n w_t r_t \bigg| \left|\sum_{t=1}^n r_t\right| \geq 2 k\right] P\left(\left|\sum_{t=1}^n r_t \right|\geq 2 k\right),
\end{align*}
hence
\begin{align*}
\E\left[\sum_{t=1}^n w_t r_t \bigg| \left|\sum_{t=1}^n r_t\right| \geq 2 k\right]
&\leq \frac{\epsilon}{P\left(\left|\sum_{t=1}^n r_t\right|\geq 2 k\right)} -\epsilon
= \frac{\epsilon}{2 P\left(\sum_{t=1}^n r_t \geq 2 k\right)} -\epsilon\\
&= \frac{\epsilon}{2 P\left(\sum_{t=1}^n \frac{r_t + 1}{2} \geq \frac{1}{2}n+ k \right)}-\epsilon.
\end{align*}
Notice that $\frac{r_t + 1}{2}$ are Bernoulli random variables.
Using Theorem~\ref{lemma:bin}, we have
\begin{align*}
\E\left[\sum_{t=1}^n w_t r_t \bigg| \left|\sum_{t=1}^n r_t\right| \geq 2 k\right]
&\leq \epsilon \left(\exp\left(\frac{1}{6}\right)\sqrt{2 \pi}\frac{2k}{\sqrt{n}} +2\exp\left(\frac{1}{6}\right)-1\right)\exp\left(n \, D\left(\frac{1}{2}+\frac{k}{n}\middle\|\frac{1}{2}\right)\right)~. \qedhere
\end{align*}
\end{proof}

\subsection{Proof of Theorem~\ref{theo:cocob}}

\begin{proof}
%Define $\theta_{t-1}=\sum_{i=1}^t g_i$.

% \subsection{Data independent bound}
% 
% We will make use of the following lower bound
% \begin{lemma}
% There exist $a,b$ such that 
% $\log(1+x) \geq x - b x^2$, for $-2/a \leq x \leq 2/a$, $a>2$, $a\geq4 b$.
% \end{lemma}
% \begin{proof}
% For example, we can set $a>3.44$ and $b=a/4$ and verify the inequality numerically.
% \end{proof}
% Note that the inequality above is equivalent to $1+x \geq \exp(x - b x^2)$.
% 
% Assume that $L_{t-1}\geq \epsilon \exp(\frac{\theta_{t-1}^2}{a(t-1)}-\sum_{i=1}^{t-1} \frac{1}{a i})$.
% We have to prove that $L_{t}\geq \epsilon \exp(\frac{\theta_{t}^2}{a t}-\sum_{i=1}^{t} \frac{1}{a i})$.
% \begin{align}
% L_{t} &= L_{t-1} (1+\beta_t g_t) \\
% &\geq (1+\beta_t g_t) \epsilon \exp(\frac{\theta_{t-1}^2}{a(t-1)}-\sum_{i=1}^{t-1} \frac{1}{a i}) \\
% &=  \epsilon \exp(\frac{\theta_{t-1}^2}{a(t-1)}+\log(1+\beta_t g_t)-\sum_{i=1}^{t-1} \frac{1}{a i}).
% \end{align}
% Hence, we lower bound the quantity $\frac{\theta_{t-1}^2}{a(t-1)}+\log(1+\beta_t g_t) - \frac{\theta_{t}^2}{a t}-\sum_{i=1}^{t-1} \frac{1}{a i}$.
% \begin{align}
% &\frac{\theta_{t-1}^2}{a(t-1)}+\log(1+\beta_t g_t) -\frac{\theta_{t-1}^2 + 2 \theta_{t-1} g_t +g_t^2}{a t} -\sum_{i=1}^{t-1} \frac{1}{a i}\\ 
% & \quad = \frac{\theta_{t-1}^2}{a} (\frac{1}{t-1}-\frac{1}{t}) + \log(1+\beta_t g_t) -\frac{2 \theta_{t-1} g_t +g_t^2}{a t} -\sum_{i=1}^{t-1} \frac{1}{a i}\\
% & \quad \geq \frac{\theta_{t-1}^2}{a\,t\,(t-1)} + \log(1+\beta_t g_t) -\frac{2 \theta_{t-1} g_t}{a t} -\sum_{i=1}^{t} \frac{1}{a i}\\
% & \quad \geq \frac{\theta_{t-1}^2}{a\,t\,(t-1)} + \beta_t g_t - b \, (\beta_t g_t)^2 -\frac{\theta_{t-1} g_t}{a t} -\sum_{i=1}^{t} \frac{1}{a i}.
% \end{align}
% If we set $\beta_t=\frac{2 \theta_{t-1}}{a t}$ we have
% \begin{align}
% &\frac{\theta_{t-1}^2}{a(t-1)}+\log(1+\beta_t g_t) -\frac{\theta_{t-1}^2 + 2 \theta_{t-1} g_t +g_t^2}{a t} -\sum_{i=1}^{t-1} \frac{1}{a i}\\ 
% & \quad \geq \frac{\theta_{t-1}^2}{a\,t\,(t-1)}  - b\frac{4\theta_{t-1}^2}{a^2 t^2} -\sum_{i=1}^{t} \frac{1}{a i} \\
% & \quad \geq -\sum_{i=1}^{t} \frac{1}{a i}
% \end{align}
% where we used the fact that $a>4 b$ that is $1/a> 4 b/ a^2$.
% Hence, by induction, we have
% \begin{align}
% L_{T} &\geq \epsilon \exp(\frac{\theta_{T}^2}{a T}-\sum_{i=1}^{T} \frac{1}{a i}) \\
% &\geq \epsilon \exp(\frac{\theta_{T}^2}{a T} - \frac{1}{a}\log(T) -\frac{1}{a}) \\
% &=\frac{\epsilon \exp(-\frac{1}{a})}{T^\frac{1}{a}}\exp(\frac{\theta_{T}^2}{a T})
% \end{align}
% 
% \subsection{Data dependent bound}
% 
% Assume that $|g_t|\leq G_t$ and that you receive at each round $t$ the quantity $G_t$ before making the bet.
% 
% Define $a_t=2 \max_{i\leq t} G_i$, $\sg_t = \sum_{i=1}^t |g_i| + \delta$ and $\theta_t=\sum_{i=1}^t g_i$.
% 
% Assume that $L_{t-1}\geq \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t-1} \sg_{t-1}}- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i})$.
% We have to prove that $L_{t}\geq \epsilon \exp(\frac{\theta_{t}^2}{a_{t} \sg_t}- \sum_{i=1}^{t} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i}))$.
% \begin{align}
% L_{t} &= L_{t-1} (1+\beta_t g_t) \\
% &\geq (1+\beta_t g_t) \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t-1} \sg_{t-1}}- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i}) \\
% &=  \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t-1} \sg_{t-1}}+\log(1+\beta_t g_t)- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i}) \\
% &\geq  \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t} \sg_{t-1}}+\log(1+\beta_t g_t)- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i}).
% \end{align}
% Consider the function 
% \[
% \phi(x)=-\log(1+\beta_t x) + \frac{(\theta_{t-1}+x)^2}{a_t \sg_{t-1} + a_t |x|}.
% \]
% We have that $\phi(x)$ is piece-wise convex on $[-\infty,0]$ and $[0,\infty]$. Hence, we have that
% \begin{align}
% &\phi(x) \leq \phi(0)+\frac{x}{G_t} (\phi(G_t)-\phi(0)), \forall 0 \leq x\leq G_t\\
% &\phi(x) \leq \phi(0)+\frac{x}{G_t} (\phi(0)-\phi(-G_t)), \forall -G_t \leq x\leq 0.
% \end{align}
% 
% We now use set $\beta_t$ such that $\phi(G_t)=\phi(-G_t)$, that is
% \[
% \beta_t = \frac{1}{G_t} \frac{A_{t-1}-1}{A_{t-1}+1} 
% = \frac{1}{G_t} \left(2 \, sigmoid\left(\frac{4 \theta_{t-1} G_t}{a_{t} \sg_{t-1} + a_t G_t}\right)-1\right)
% \]
% where $A_{t-1}=\exp\left(\frac{4 \theta_{t-1} G_t}{a_{t} \sg_{t-1} + a_t G_t}\right)$ and
% $sigmoid (x) =\frac{1}{1+\exp(-x)}$.
% Hence we have
% \[
% \phi(x) \leq \phi(0)+\frac{|x|}{G_t} (\phi(G_t)-\phi(0)), \forall -G_t \leq x\leq G_t
% \]
% that is
% \begin{align}
% &\frac{\theta_{t-1}^2}{a_t \sg_{t-1}}-\frac{(\theta_{t-1}+x)^2}{a_t \sg_{t-1}+a_t |x|} + \log(1+\beta_t g_t) = \phi(0) - \phi(x) 
% \geq \frac{|x|}{G_t} (\phi(0) - \phi(G_t)) \\
% &\quad = \frac{|x|}{G_t} (\frac{\theta_{t-1}^2}{a_t \sg_{t-1}} - \frac{(\theta_{t-1}+G_t)^2}{a_t \sg_{t-1} + a_t G_t} + \log(1+\beta_t G_t)), \forall -G_t \leq x\leq G_t.
% \end{align}
% 
% 
% Using this relation we have that
% \begin{align}
% &-\frac{(\theta_{t-1}+g_t)^2}{a_t \sg_{t-1}+a_t |g_t|} + \frac{\theta_{t-1}^2}{a_t \sg_{t-1}}+\log(1+\beta_t g_t)-\sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i} \\
% &\quad \geq \frac{|g_t|}{G_t} (\frac{\theta_{t-1}^2}{a_t \sg_{t-1}} - \frac{(\theta_{t-1}+G_t)^2}{a_t \sg_{t-1} + a_t G_t} + \log(1+\beta_t G_t)) - \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i}\\
% &\quad = \frac{|g_t|}{G_t} (\frac{a_t G_t \theta_{t-1}^2 -2 G_t \theta_{t-1} \sg_{t-1} }{a_t \sg_{t-1}(a_t \sg_{t-1} + a_t G_t)} + \log(1+\beta_t G_t)) - \sum_{i=1}^{t} \frac{|g_i|G_i}{a_i \sg_{i-1} + a_i G_i}\\
% &\quad \geq \frac{|g_t|}{G_t} (\frac{a_t G_t \theta_{t-1}^2}{a_t \sg_{t-1}(a_t \sg_{t-1} + a_t G_t)}-\frac{2 G_t \theta_{t-1}}{a_t \sg_{t-1} + a_t G_t} + \log(1+\beta_t G_t)) - \sum_{i=1}^{t} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i}.
% \end{align}
% We now use the Taylor expansion, to obtain
% \[
% \log\left(1+\frac{\exp(x)-1}{\exp(x)+1}\right) \geq \frac{x}{2} -\frac{x^2}{8} \qquad \forall x \in \R
% \]
% and, using the expression of $\beta_t$, have
% \[
% \log\left(1+\beta_t G_t\right) = \log\left(1+\frac{\exp\left(\frac{4 \theta_{t-1} G_t}{a_t \sg_{t-1} + a_t G_t}\right)-1}{\exp\left(\frac{4 \theta_{t-1} G_t}{a_t \sg_{t-1} + a_t G_t}\right)+1}\right) \geq \frac{2 \theta_{t-1} G_t}{a_t \sg_{t-1} + a_t G_t} -\frac{2 \theta_{t-1}^2 G_t^2}{(a_t \sg_{t-1} + a_t G_t)^2}.
% \]
% Hence the expression 
% \[
% \frac{a_t G_t \theta_{t-1}^2}{a_t \sg_{t-1}(a_t \sg_{t-1} + a_t G_t)}-\frac{2 G_t \theta_{t-1}}{a_t \sg_{t-1} + a_t G_t} + \log(1+\beta_t G_t))
% \]
% is greater than zero if $a_t \geq 2 G_t$, that is true by definition of $a_t$.
% 
% By induction, the final lower bound is 
% \[
% L_{T} \geq \epsilon \exp\left(\frac{\theta_{T}^2}{a_T \sg_{T-1} + a_T G_T}-\sum_{i=1}^{T} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i G_i} \right) \\
% \]

%\subsection{Data dependent bound, version 2}

From the assumptions we have that $|g_t|\leq 1$.

%Define $\sg_t = \sum_{i=1}^t |g_i| G_i + \delta$ and $\theta_t=\sum_{i=1}^t g_i$.
We will prove by induction a lower bound on $\gain_n$, hence our induction hypothesis is
\[
\gain_{t-1}\geq \epsilon \exp\left(\frac{\theta_{t-1}^2}{a \sg_{t-1}}- \sum_{i=1}^{t-1} \frac{|g_i|}{a (\sg_{i-1} + 1)}\right)~.
\]
We have to prove that 
\[
\gain_{t}\geq \epsilon \exp\left(\frac{\theta_{t}^2}{a \sg_t}- \sum_{i=1}^{t} \frac{|g_i| G_i}{a (\sg_{i-1} + 1)}\right)~.
\]

Let's start calculating the value of the reward at time $t$.
\begin{align*}
\gain_{t} &= \gain_{t-1} (1+\beta_t g_t) \\
&\geq \epsilon (1+\beta_t g_t)\exp\left(\frac{\theta_{t-1}^2}{a \sg_{t-1}}- \sum_{i=1}^{t-1} \frac{|g_i|}{a (\sg_{i-1} + 1) }\right) \\
&=  \epsilon \exp\left(\frac{\theta_{t-1}^2}{a \sg_{t-1}}+\log(1+\beta_t g_t)- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a (\sg_{i-1} + 1)}\right)~.
\end{align*}

Now, consider the function 
\[
\phi(x)=-\log(1+\beta_t x) + \frac{(\theta_{t-1}+x)^2}{a (\sg_{t-1} + |x|)}~.
\]
We have that $\phi(x)$ is piece-wise convex on $[-\infty,0]$ and $[0,\infty]$. Hence, we have that
\begin{align*}
&\phi(x) \leq \phi(0)+x (\phi(1)-\phi(0)), \forall 0 \leq x\leq 1\\
&\phi(x) \leq \phi(0)+x (\phi(0)-\phi(-1)), \forall -1 \leq x\leq 0~.
\end{align*}
Also, we set $\beta_t$ such that $\phi(1)=\phi(-1)$, that is
\[
\beta_t = \frac{A_{t-1}-1}{A_{t-1}+1} 
= 2 \, S\left(\frac{4 \theta_{t-1}}{a (\sg_{t-1} + 1)}\right)-1
\]
where $A_{t-1}=\exp\left(\frac{4 \theta_{t-1}}{a (\sg_{t-1} + 1)}\right)$ and
$S(x) =\frac{1}{1+\exp(-x)}$.
Hence we have
\[
\phi(x) \leq \phi(0)+ |x| (\phi(1)-\phi(0)), \forall -1 \leq x\leq 1,
\]
that is
\begin{align*}
\frac{\theta_{t-1}^2}{a \sg_{t-1}}-\frac{(\theta_{t-1}+x)^2}{a (\sg_{t-1} +  |x|)} + \log(1+\beta_t g_t) 
& = \phi(0) - \phi(x) \\
& \geq |x| \left(\phi(0) - \phi(1)\right) \\
&= |x| \left(\frac{\theta_{t-1}^2}{a \sg_{t-1}} - \frac{(\theta_{t-1}+G_t)^2}{a (\sg_{t-1} + 1)} + \log(1+\beta_t)\right), \forall -1 \leq x\leq 1~.
\end{align*}

Using this relation we have that
\begin{align*}
&-\frac{(\theta_{t-1}+g_t)^2}{a (\sg_{t-1}+|g_t|)} + \frac{\theta_{t-1}^2}{a \sg_{t-1}}+\log(1+\beta_t g_t)-\sum_{i=1}^{t-1} \frac{|g_i|}{a (\sg_{i-1} + 1)} \\
&\qquad \geq |g_t| \left(\frac{\theta_{t-1}^2}{a \sg_{t-1}} - \frac{(\theta_{t-1}+1)^2}{a (\sg_{t-1} + 1)} + \log(1+\beta_t G_t)\right) - \sum_{i=1}^{t-1} \frac{|g_i|}{a (\sg_{i-1} + 1)}\\
&\qquad = |g_t| \left(\frac{a \theta_{t-1}^2 -2 \theta_{t-1} \sg_{t-1} }{a^2 \sg_{t-1}(\sg_{t-1} + 1)} + \log(1+\beta_t G_t)\right) - \sum_{i=1}^{t} \frac{|g_i|}{a (\sg_{i-1} + 1)}\\
&\qquad \geq |g_t| \left(\frac{a \theta_{t-1}^2}{a^2 \sg_{t-1}(\sg_{t-1} + 1)}-\frac{2 \theta_{t-1}}{a (\sg_{t-1} + 1)} + \log(1+\beta_t)\right) - \sum_{i=1}^{t} \frac{|g_i|}{a (\sg_{i-1} + 1)}~.
\end{align*}
We now use the Taylor expansion, to obtain
\[
\log\left(1+\frac{\exp(x)-1}{\exp(x)+1}\right) \geq \frac{x}{2} -\frac{x^2}{8} \qquad \forall x \in \R
\]
and, using the expression of $\beta_t$, have
\[
\log\left(1+\beta_t\right) 
= \log\left(1+\frac{\exp\left(\frac{4 \theta_{t-1}}{a (\sg_{t-1} + 1)}\right)-1}{\exp\left(\frac{4 \theta_{t-1}}{a (\sg_{t-1} + 1)}\right)+1}\right) 
\geq \frac{2 \theta_{t-1}}{a (\sg_{t-1} + 1)} -\frac{2 \theta_{t-1}^2}{a^2 (\sg_{t-1} + 1)^2}.
\]
Hence the expression 
\[
\frac{a \theta_{t-1}^2}{a^2 \sg_{t-1}(\sg_{t-1} + 1)}-\frac{2 \theta_{t-1}}{a (\sg_{t-1} + 1)} + \log(1+\beta_t))
\]
is greater than zero if $a \geq 2$, that is true by definition of $a$.

Hence, by induction, the stated bound on $\gain_n$ is proved.
\end{proof}

% \subsection{Data dependent bound, version 3}
% 
% Assume that $|g_t|\leq G_t$ and that you receive at each round $t$ the quantity $G_t$ before making the bet.
% 
% Define $\sg_t = \sum_{i=1}^t |g_i| G_i$ and $\theta_t=\sum_{i=1}^t g_i$.
% 
% Assume that $L_{t-1}\geq \epsilon \exp(\frac{\theta_{t-1}^2}{a (\sg_{t-1} + \delta_{t-1})}- \frac{1}{a}\sum_{i=1}^{t-1} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_{i}})$.
% We have to prove that $L_{t}\geq \epsilon \exp(\frac{\theta_{t}^2}{a (\sg_t + \delta_t)}- \frac{1}{a}\sum_{i=1}^{t} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_i}))$.
% \begin{align}
% L_{t} &= L_{t-1} (1+\beta_t g_t) \\
% &\geq (1+\beta_t g_t) \epsilon \exp(\frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_{t-1})}- \frac{1}{a}\sum_{i=1}^{t-1} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_i}) \\
% &=  \epsilon \exp(\frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_{t-1})}+\log(1+\beta_t g_t)- \frac{1}{a}\sum_{i=1}^{t-1} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i +\delta_i}) .
% \end{align}
% Consider the function 
% \[
% \phi(x)=-\log(1+\beta_t x) + \frac{(\theta_{t-1}+x)^2}{a \sg_{t-1} + a |x| G_t +a \delta_t}.
% \]
% We have that $\phi(x)$ is piece-wise convex on $[-\infty,0]$ and $[0,\infty]$. Hence, we have that
% \begin{align}
% &\phi(x) \leq \phi(0)+\frac{x}{G_t} (\phi(G_t)-\phi(0)), \forall 0 \leq x\leq G_t\\
% &\phi(x) \leq \phi(0)+\frac{x}{G_t} (\phi(0)-\phi(-G_t)), \forall -G_t \leq x\leq 0.
% \end{align}
% 
% We now use set $\beta_t$ such that $\phi(G_t)=\phi(-G_t)$, that is
% \[
% \beta_t = \frac{1}{G_t} \frac{A_{t-1}-1}{A_{t-1}+1} 
% = \frac{1}{G_t} \left(2 \, sigmoid\left(\frac{4 \theta_{t-1} G_t}{a \sg_{t-1} + a G_t^2 + a \delta_t}\right)-1\right)
% \]
% where $A_{t-1}=\exp\left(\frac{4 \theta_{t-1} G_t}{a_{t} \sg_{t-1} + a G_t + a \delta_t }\right)$ and
% $sigmoid (x) =\frac{1}{1+\exp(-x)}$.
% Hence we have
% \[
% \phi(x) \leq \phi(0)+\frac{|x|}{G_t} (\phi(G_t)-\phi(0)), \forall -G_t \leq x\leq G_t
% \]
% that is
% \begin{align}
% &\frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_{t-1})}-\frac{(\theta_{t-1}+x)^2}{a \sg_{t-1} + a |x| G_t + a\delta_t} + \log(1+\beta_t g_t) \\
% &\quad \geq \frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_{t})}-\frac{(\theta_{t-1}+x)^2}{a \sg_{t-1} + a |x| G_t + a\delta_t} + \log(1+\beta_t g_t) \\
% &\quad = \phi(0) - \phi(x) 
% \geq \frac{|x|}{G_t} (\phi(0) - \phi(G_t)) \\
% &\quad = \frac{|x|}{G_t} (\frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_t)} - \frac{(\theta_{t-1}+G_t)^2}{a \sg_{t-1} + a G_t^2 + a \delta_t} + \log(1+\beta_t G_t)), \forall -G_t \leq x\leq G_t.
% \end{align}
% 
% Using this relation we have that
% \begin{align}
% &-\frac{(\theta_{t-1}+g_t)^2}{a (\sg_{t-1}+|g_t| G_t+\delta_t)} + \frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_{t-1})}+\log(1+\beta_t g_t)-\frac{1}{a}\sum_{i=1}^{t-1} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_i} \\
% &\quad \geq \frac{|g_t|}{G_t} (\frac{\theta_{t-1}^2}{a (\sg_{t-1}+\delta_t)} - \frac{(\theta_{t-1}+G_t)^2}{a (\sg_{t-1} + G^2_t+ \delta_t)} + \log(1+\beta_t G_t)) - \frac{1}{a}\sum_{i=1}^{t-1} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_i}\\
% &\quad = \frac{|g_t|}{G_t} (\frac{G^2_t \theta_{t-1}^2 -2 G_t \theta_{t-1} (\sg_{t-1} +\delta_t)}{a (\sg_{t-1}+\delta_t)(\sg_{t-1} + G^2_t +\delta_t)} + \log(1+\beta_t G_t)) - \frac{1}{a}\sum_{i=1}^{t} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_i}\\
% &\quad = \frac{|g_t|}{G_t} (\frac{G^2_t \theta_{t-1}^2}{a (\sg_{t-1}+\delta_t)(\sg_{t-1} + G^2_t +\delta_t)}-\frac{2 G_t \theta_{t-1}}{a (\sg_{t-1} + G^2_t+\delta_t)} + \log(1+\beta_t G_t)) - \frac{1}{a}\sum_{i=1}^{t} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i + \delta_i}.
% \end{align}
% We now use the Taylor expansion, to obtain
% \[
% \log\left(1+\frac{\exp(x)-1}{\exp(x)+1}\right) \geq \frac{x}{2} -\frac{x^2}{8} \qquad \forall x \in \R
% \]
% and, using the expression of $\beta_t$, have
% \[
% \log\left(1+\beta_t G_t\right) 
% = \log\left(1+\frac{\exp\left(\frac{4 \theta_{t-1} G_t}{a (\sg_{t-1} + G^2_t+\delta_t)}\right)-1}{\exp\left(\frac{4 \theta_{t-1} G_t}{a (\sg_{t-1} + G^2_t+\delta_t)}\right)+1}\right) 
% \geq \frac{2 \theta_{t-1} G_t}{a (\sg_{t-1} + G^2_t+\delta_t)} -\frac{2 \theta_{t-1}^2 G_t^2}{a^2 (\sg_{t-1} + G^2_t+\delta_t)^2}.
% \]
% Hence the expression 
% \[
% \frac{G^2_t \theta_{t-1}^2}{a (\sg_{t-1}+\delta_t)(\sg_{t-1} + G^2_t +\delta_t)}-\frac{2 G_t \theta_{t-1}}{a (\sg_{t-1} + G^2_t+\delta_t)} + \log(1+\beta_t G_t))
% \]
% is greater than zero if $a \geq 2$, that is true by definition of $a$.
% 
% By induction, the final lower bound is 
% \[
% L_{T} \geq \epsilon \exp\left(\frac{\theta_{T}^2}{a (\sg_{T}+\delta_T)} - \frac{1}{a}\sum_{i=1}^{T} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i +\delta_i} \right) \\
% \]
% 
% %We now upper bound the last term in the last inequality:
% %\begin{align}
% %\sum_{i=1}^{T} \frac{|g_i| G_i}{\sg_{i-1} + G^2_i +\delta_i}
% %&\leq \sum_{i=1}^{T} \frac{|g_i| G_i}{\sg_{i} + \delta_i} \\
% %&\leq \log\left(1+ \frac{\sum_{i=1}^t |g_i| G_i}{\delta_T}\right) + \log\frac{\delta_T}{\delta_1}.
% %\end{align}
% 
% We can set $\delta_t$ such that $\frac{|g_i| G_i}{\sg_{i-1} + G^2_i +\delta_i}\leq \frac{1}{i}$ and $\delta_i$ are increasing.
% 
% 
% \subsection{Data dependent bound, no $\delta$}
% 
% Assume that $|g_t|\leq G_t$ and that you receive at each round $t$ the quantity $G_t$ before making the bet.
% 
% Define $a_t=2 \max_{i\leq t} G_i^2$, $\sg_t = \sum_{i=1}^t \frac{|g_i|}{G_t} + 1$ and $\theta_t=\sum_{i=1}^t g_i$.
% 
% Assume that $L_{t-1}\geq \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t-1} \sg_{t-1}}- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i})$.
% We have to prove that $L_{t}\geq \epsilon \exp(\frac{\theta_{t}^2}{a_{t} \sg_t}- \sum_{i=1}^{t} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i}))$.
% \begin{align}
% L_{t} &= L_{t-1} (1+\beta_t g_t) \\
% &\geq (1+\beta_t g_t) \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t-1} \sg_{t-1}}- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i }) \\
% &=  \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t-1} \sg_{t-1}}+\log(1+\beta_t g_t)- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i}) \\
% &\geq  \epsilon \exp(\frac{\theta_{t-1}^2}{a_{t} \sg_{t-1}}+\log(1+\beta_t g_t)- \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i}).
% \end{align}
% Consider the function 
% \[
% \phi(x)=-\log(1+\beta_t x) + \frac{(\theta_{t-1}+x)^2}{a_t \sg_{t-1} + a_t \frac{|x|}{G_t}}.
% \]
% We have that $\phi(x)$ is piece-wise convex on $[-\infty,0]$ and $[0,\infty]$. Hence, we have that
% \begin{align}
% &\phi(x) \leq \phi(0)+\frac{x}{G_t} (\phi(G_t)-\phi(0)), \forall 0 \leq x\leq G_t\\
% &\phi(x) \leq \phi(0)+\frac{x}{G_t} (\phi(0)-\phi(-G_t)), \forall -G_t \leq x\leq 0.
% \end{align}
% 
% We now use set $\beta_t$ such that $\phi(G_t)=\phi(-G_t)$, that is
% \[
% \beta_t = \frac{1}{G_t} \frac{A_{t-1}-1}{A_{t-1}+1} 
% = \frac{1}{G_t} \left(2 \, sigmoid\left(\frac{4 \theta_{t-1} G_t}{a_{t} \sg_{t-1} + a_t}\right)-1\right)
% \]
% where $A_{t-1}=\exp\left(\frac{4 \theta_{t-1} G_t}{a_{t} \sg_{t-1} + a_t}\right)$ and
% $sigmoid (x) =\frac{1}{1+\exp(-x)}$.
% Hence we have
% \[
% \phi(x) \leq \phi(0)+\frac{|x|}{G_t} (\phi(G_t)-\phi(0)), \forall -G_t \leq x\leq G_t
% \]
% that is
% \begin{align}
% &\frac{\theta_{t-1}^2}{a_t \sg_{t-1}}-\frac{(\theta_{t-1}+x)^2}{a_t \sg_{t-1}+a_t \frac{|x|}{G_t}} + \log(1+\beta_t g_t) = \phi(0) - \phi(x) 
% \geq \frac{|x|}{G_t} (\phi(0) - \phi(G_t)) \\
% &\quad = \frac{|x|}{G_t} (\frac{\theta_{t-1}^2}{a_t \sg_{t-1}} - \frac{(\theta_{t-1}+G_t)^2}{a_t \sg_{t-1} + a_t} + \log(1+\beta_t G_t)), \forall -G_t \leq x\leq G_t.
% \end{align}
% 
% 
% Using this relation we have that
% \begin{align}
% &-\frac{(\theta_{t-1}+g_t)^2}{a_t \sg_{t-1}+a_t \frac{|g_t|}{G_t}} + \frac{\theta_{t-1}^2}{a_t \sg_{t-1}}+\log(1+\beta_t g_t)-\sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i} \\
% &\quad \geq \frac{|g_t|}{G_t} (\frac{\theta_{t-1}^2}{a_t \sg_{t-1}} - \frac{(\theta_{t-1}+G_t)^2}{a_t \sg_{t-1} + a_t} + \log(1+\beta_t G_t)) - \sum_{i=1}^{t-1} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i}\\
% &\quad = \frac{|g_t|}{G_t} (\frac{\theta_{t-1}^2 -2 G_t \theta_{t-1} \sg_{t-1} }{a_t \sg_{t-1}(\sg_{t-1} + 1)} + \log(1+\beta_t G_t)) - \sum_{i=1}^{t} \frac{|g_i|G_i}{a_i \sg_{i-1} + a_i}\\
% &\quad = \frac{|g_t|}{G_t} (\frac{\theta_{t-1}^2}{a_t \sg_{t-1}(\sg_{t-1} + 1)}-\frac{2 G_t \theta_{t-1}}{a_t \sg_{t-1} + a_t} + \log(1+\beta_t G_t)) - \sum_{i=1}^{t} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i}.
% \end{align}
% We now use the Taylor expansion, to obtain
% \[
% \log\left(1+\frac{\exp(x)-1}{\exp(x)+1}\right) \geq \frac{x}{2} -\frac{x^2}{8} \qquad \forall x \in \R
% \]
% and, using the expression of $\beta_t$, have
% \[
% \log\left(1+\beta_t G_t\right) = \log\left(1+\frac{\exp\left(\frac{4 \theta_{t-1} G_t}{a_t \sg_{t-1} + a_t}\right)-1}{\exp\left(\frac{4 \theta_{t-1} G_t}{a_t \sg_{t-1} + a_t}\right)+1}\right) \geq \frac{2 \theta_{t-1} G_t}{a_t \sg_{t-1} + a_t} -\frac{2 \theta_{t-1}^2 G_t^2}{(a_t \sg_{t-1} + a_t)^2}.
% \]
% Hence the expression 
% \[
% \frac{\theta_{t-1}^2}{a_t \sg_{t-1}(\sg_{t-1} + 1)}-\frac{2 G_t \theta_{t-1}}{a_t \sg_{t-1} + a_t} + \log(1+\beta_t G_t))
% \]
% is greater than zero if $a_t \geq 2 G_t^2$, that is true by definition of $a_t$.
% 
% By induction, the final lower bound is 
% \begin{align}
% L_{T} 
% &\geq \epsilon \exp\left(\frac{\theta_{T}^2}{a_T \sg_{T}}-\sum_{i=1}^{T} \frac{|g_i| G_i}{a_i \sg_{i-1} + a_i} \right) \\
% &\geq \epsilon \exp\left(\frac{\theta_{T}^2}{2 G_T^2 \sg_{T}}-\frac{1}{2}\sum_{i=1}^{T} \frac{\frac{|g_i|}{G_i}}{\sg_{i}} \right) \\
% &\geq \epsilon \exp\left(\frac{\theta_{T}^2}{2 G_T^2 \sg_{T}} - \frac{1}{2} \log\left(1+ \sum_{i=1}^{T} \frac{|g_i|}{G_i} \right) \right)
% \end{align}
% 
% It is interesting to note that $\beta_t$ is \emph{not} between $-1$ and $1$, but between $-1/G_t$ and $1/G_t$.
% 
% The ideal case of $\exp\left(\frac{\theta_T^2}{\sum_{t=1}^T g_t^2}\right)$ is equal to
% \[
% \exp\left(\frac{\theta_T^2}{T\frac{\sum_{t=1}^T g_t^2}{T}}\right) 
% \approx \exp\left(\frac{\theta_T^2}{\left(\sum_{t=1}^T \frac{|g_t|}{G_t}\right) \frac{\sum_{t=1}^T g_t^2}{T}}\right)
% \approx \exp\left(\frac{\theta_T^2}{\left(\sum_{t=1}^T \frac{|g_t|}{G_t}\right) \max_{t \leq T} g_t^2 }\right)
% \]
