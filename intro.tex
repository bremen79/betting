\section{Introduction}
\label{sec:intro}

\acresetall

\ac{OCO} is a problem where an algorithm repeatedly chooses a point $w_t$ from a convex decision set $K$, observes an arbitrary, or even adversarially chosen, convex loss function $\ell_t$ and suffers loss $\ell_t(w_t)$. The goal of the algorithm is to have a small cumulative loss. Performance of an algorithm is evaluated by the so-called regret, which is the difference of cumulative losses of the algorithm and of the (hypothetical) strategy that would choose in every round the same best point in hindsight.
Typically, one tries to prove that the regret grows at most sub-linearly in time, that is, the average regret vanishes over time.

Typically, \ac{OCO} is solved with a reduction of a \ac{OLO} problem~\citep{Cesa-BianchiL06,Shalev-Shwartz12}, where the losses $\ell_t(w)$ are the linear functions $\langle w, g_t\rangle$.
Indeed, many learning problems can be directly phrased as \ac{OLO}, e.g., learning with expert advice~\citep{LittlestoneW94,Vovk98,Cesa-BianchiFHHSW97}, online combinatorial optimization~\cite{KoolenWK10}. A part from \ac{OCO}, other problems can be also reduced to \ac{OLO}, e.g. online classification and regression~\citep[Chapters~11~and~12]{Cesa-BianchiL06}, multi-armed problems~\citep[Chapter~6]{Cesa-BianchiL06}, and batch and stochastic optimization of convex functions~\cite{NemirovskyY83}.  Hence, a result in \ac{OLO} immediately implies other results in all these domains.

However, as essential as it is, being able to solve an optimization problem is only half of the problem in machine learning. In fact, we are often interested in the generalization performance of a predictor trained from data. This depends not only on the ability to optimize a loss but also on constraining the complexity of the predictor. This is usually achieved through the use an regularized objective function, that biases the solution towards a small region of the space. However, choosing the size of this region is usually a problem by itself, that is solved by practitioners with a variety of more or less theoretical principled tools.

In this paper, we claim that a more fundamental notion subsumes both \ac{OLO} and \ac{OCO}. This notion is linked to the ability of an algorithm to optimally bet on an arbitrary sequence of outcomes from a coin. We will define the notion of \ac{MBA} that not only will allow us to solve \ac{OLO} and \ac{OCO}, but to design algorithm that do not require any explicit form of regularization nor any hyper-parameter to tune, yet are able to achieve optimal worst case guarantees. 
In particular, we will show that an algorithm that guarantees a reward close to the optimal sequence of bets also guarantees optimal regret in \ac{OLO}/\ac{OCO} and automatic model selection in regularized \ac{ERM}.

We will also show connections between the optimal the betting strategy known in economics as Kelly betting \citep{Kelly56} and online learning, and hence indirectly with stochastic optimization and statistical learning.